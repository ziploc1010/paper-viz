{
  "id": "lrm-hierarchy-model",
  "title": "Large Reasoning Models (LRM) Efficiency Hierarchy",
  "subtitle": "Interactive Learning Experience for Efficient Reasoning Methods",
  "description": "Explore the comprehensive taxonomy of efficient reasoning methods in Large Reasoning Models through interactive diagrams and coding exercises",
  "svgDiagram": "<svg viewBox=\"0 0 1400 1200\" xmlns=\"http://www.w3.org/2000/svg\">\n  <!-- Define styles -->\n  <defs>\n    <style>\n      .title-box { fill: #D8D8F6; stroke: #9999CC; stroke-width: 1; }\n      .main-box { fill: #C8C8E8; stroke: #8888BB; stroke-width: 1; }\n      .inference-box { fill: #FFE4B5; stroke: #DAA520; stroke-width: 1; cursor: pointer; }\n      .inference-box:hover { fill: #FFD8A0; }\n      .sft-box { fill: #E0D5F7; stroke: #8B7AA8; stroke-width: 1; cursor: pointer; }\n      .sft-box:hover { fill: #D5C8F0; }\n      .rl-box { fill: #FFE4E1; stroke: #CD5C5C; stroke-width: 1; cursor: pointer; }\n      .rl-box:hover { fill: #FFD8D4; }\n      .pretrain-box { fill: #B0E0E6; stroke: #4682B4; stroke-width: 1; cursor: pointer; }\n      .pretrain-box:hover { fill: #A0D4DB; }\n      .subquad-box { fill: #D4F1D4; stroke: #228B22; stroke-width: 1; cursor: pointer; }\n      .subquad-box:hover { fill: #C8E8C8; }\n      .future-box { fill: #FFDAB9; stroke: #CD853F; stroke-width: 1; cursor: pointer; }\n      .future-box:hover { fill: #FFCEAD; }\n      .text-main { font-family: Arial, sans-serif; font-size: 11px; fill: #333; font-weight: 500; }\n      .text { font-family: Arial, sans-serif; font-size: 9px; fill: #333; }\n      .text-small { font-family: Arial, sans-serif; font-size: 8px; fill: #333; }\n      .connector { stroke: #666; stroke-width: 1; fill: none; }\n    </style>\n  </defs>\n\n  <!-- Title -->\n  <rect x=\"20\" y=\"20\" width=\"220\" height=\"1160\" class=\"title-box\" rx=\"10\"/>\n  <text x=\"130\" y=\"600\" text-anchor=\"middle\" class=\"text-main\" transform=\"rotate(-90 130 600)\">Hierarchy of Efficient Reasoning Methods in Large Reasoning Models (LRMs)</text>\n\n  <!-- I. Efficient Reasoning during Inference -->\n  <rect x=\"270\" y=\"50\" width=\"180\" height=\"30\" class=\"main-box\" rx=\"5\"/>\n  <text x=\"360\" y=\"69\" text-anchor=\"middle\" class=\"text-main\">I. Efficient Reasoning during Inference</text>\n  <line x1=\"240\" y1=\"65\" x2=\"270\" y2=\"65\" class=\"connector\"/>\n  \n  <!-- 1. Length Budgeting -->\n  <rect x=\"480\" y=\"30\" width=\"100\" height=\"25\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"47\" text-anchor=\"middle\" class=\"text\">Length Budgeting</text>\n  <line x1=\"450\" y1=\"65\" x2=\"470\" y2=\"65\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"65\" x2=\"470\" y2=\"42\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"42\" x2=\"480\" y2=\"42\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"length-budgeting-papers\">\n    <rect x=\"600\" y=\"10\" width=\"320\" height=\"90\" class=\"inference-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"25\" class=\"text\">TALE (Han et al., 2024)</text>\n    <text x=\"610\" y=\"38\" class=\"text\">Sketch-of-Thought (Aytes et al., 2025)</text>\n    <text x=\"610\" y=\"51\" class=\"text\">Planning Tokens (Wang et al., 2023b)</text>\n    <text x=\"610\" y=\"64\" class=\"text\">Chain of Draft (Xu et al., 2025a)</text>\n    <text x=\"610\" y=\"77\" class=\"text\">S1 (Muennighoff et al., 2025)</text>\n    <text x=\"610\" y=\"90\" class=\"text\">SafeChain (Jiang et al., 2025)</text>\n    <text x=\"760\" y=\"25\" class=\"text\">DSC (Wang et al., 2024b)</text>\n    <text x=\"760\" y=\"38\" class=\"text\">Dynasor (Fu et al., 2024b)</text>\n    <text x=\"760\" y=\"51\" class=\"text\">TSP (Wang et al., 2025e)</text>\n  </g>\n  <line x1=\"580\" y1=\"42\" x2=\"600\" y2=\"42\" class=\"connector\"/>\n  \n  <!-- 2. System Switch -->\n  <rect x=\"480\" y=\"110\" width=\"100\" height=\"35\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"123\" text-anchor=\"middle\" class=\"text\">System Switch</text>\n  <text x=\"530\" y=\"135\" text-anchor=\"middle\" class=\"text-small\">(System 1 ↔ System 2)</text>\n  <line x1=\"470\" y1=\"65\" x2=\"470\" y2=\"127\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"127\" x2=\"480\" y2=\"127\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"system-switch-papers\">\n    <rect x=\"600\" y=\"110\" width=\"320\" height=\"50\" class=\"inference-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"125\" class=\"text\">Dualformer (Su et al., 2024a)</text>\n    <text x=\"610\" y=\"138\" class=\"text\">System 1.x (Saha et al., 2024)</text>\n    <text x=\"610\" y=\"151\" class=\"text\">FaST (Sun et al., 2025a)</text>\n    <text x=\"760\" y=\"125\" class=\"text\">HaluSearch (Cheng et al., 2025a)</text>\n    <text x=\"760\" y=\"138\" class=\"text\">Dyna-Think (Pan et al., 2024)</text>\n  </g>\n  <line x1=\"580\" y1=\"127\" x2=\"600\" y2=\"127\" class=\"connector\"/>\n  \n  <!-- 3. Model Switch -->\n  <rect x=\"480\" y=\"170\" width=\"100\" height=\"25\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"187\" text-anchor=\"middle\" class=\"text\">Model Switch</text>\n  <line x1=\"470\" y1=\"65\" x2=\"470\" y2=\"182\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"182\" x2=\"480\" y2=\"182\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"model-switch-papers\">\n    <rect x=\"600\" y=\"170\" width=\"320\" height=\"65\" class=\"inference-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"185\" class=\"text\">BiLD (Kim et al., 2023)</text>\n    <text x=\"610\" y=\"198\" class=\"text\">EAGLE / EAGLE-2 (Li et al., 2024e, 2024d)</text>\n    <text x=\"610\" y=\"211\" class=\"text\">MEDUSA (Cai et al., 2024)</text>\n    <text x=\"610\" y=\"224\" class=\"text\">LayerSkip (Elhoushi et al., 2024)</text>\n    <text x=\"760\" y=\"185\" class=\"text\">Zooter (Lu et al., 2023)</text>\n    <text x=\"760\" y=\"198\" class=\"text\">RouteLLM (Ong et al., 2025)</text>\n    <text x=\"760\" y=\"211\" class=\"text\">MixLLM (Wang et al., 2025b)</text>\n  </g>\n  <line x1=\"580\" y1=\"182\" x2=\"600\" y2=\"182\" class=\"connector\"/>\n  \n  <!-- 4. Parallel Search -->\n  <rect x=\"480\" y=\"245\" width=\"100\" height=\"25\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"262\" text-anchor=\"middle\" class=\"text\">Parallel Search</text>\n  <line x1=\"470\" y1=\"65\" x2=\"470\" y2=\"257\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"257\" x2=\"480\" y2=\"257\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"parallel-search-papers\">\n    <rect x=\"600\" y=\"245\" width=\"320\" height=\"65\" class=\"inference-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"260\" class=\"text\">SBoN (Sun et al., 2024a)</text>\n    <text x=\"610\" y=\"273\" class=\"text\">TreeBoN (Qiu et al., 2024)</text>\n    <text x=\"610\" y=\"286\" class=\"text\">STBoN (Wang et al., 2025d)</text>\n    <text x=\"610\" y=\"299\" class=\"text\">SelfCalibration (Huang et al., 2025a)</text>\n    <text x=\"760\" y=\"260\" class=\"text\">MetaReasoner (Sui et al., 2025)</text>\n    <text x=\"760\" y=\"273\" class=\"text\">TPO (Li et al., 2025d)</text>\n  </g>\n  <line x1=\"580\" y1=\"257\" x2=\"600\" y2=\"257\" class=\"connector\"/>\n\n  <!-- II. Efficient Reasoning with SFT -->\n  <rect x=\"270\" y=\"330\" width=\"180\" height=\"30\" class=\"main-box\" rx=\"5\"/>\n  <text x=\"360\" y=\"349\" text-anchor=\"middle\" class=\"text-main\">II. Efficient Reasoning with SFT</text>\n  <line x1=\"240\" y1=\"345\" x2=\"270\" y2=\"345\" class=\"connector\"/>\n  \n  <!-- 1. Reasoning Chain Compression -->\n  <rect x=\"480\" y=\"320\" width=\"100\" height=\"35\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"333\" text-anchor=\"middle\" class=\"text\">Reasoning Chain</text>\n  <text x=\"530\" y=\"345\" text-anchor=\"middle\" class=\"text\">Compression</text>\n  <line x1=\"450\" y1=\"345\" x2=\"470\" y2=\"345\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"345\" x2=\"470\" y2=\"337\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"337\" x2=\"480\" y2=\"337\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"chain-compression-papers\">\n    <rect x=\"600\" y=\"320\" width=\"320\" height=\"65\" class=\"sft-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"335\" class=\"text\">TokenSkip (Xia et al., 2025)</text>\n    <text x=\"610\" y=\"348\" class=\"text\">SPIRIT-FT (Cui et al., 2025)</text>\n    <text x=\"610\" y=\"361\" class=\"text\">Skip Steps (Liu et al., 2024c)</text>\n    <text x=\"610\" y=\"374\" class=\"text\">Distill System 2 (Yu et al., 2024a)</text>\n    <text x=\"760\" y=\"335\" class=\"text\">C3ot (Kang et al., 2024)</text>\n    <text x=\"760\" y=\"348\" class=\"text\">Self-Training (Munkhbat et al., 2025)</text>\n  </g>\n  <line x1=\"580\" y1=\"337\" x2=\"600\" y2=\"337\" class=\"connector\"/>\n  \n  <!-- 2. Latent-Space SFT -->\n  <rect x=\"480\" y=\"395\" width=\"100\" height=\"25\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"412\" text-anchor=\"middle\" class=\"text\">Latent-Space SFT</text>\n  <line x1=\"470\" y1=\"345\" x2=\"470\" y2=\"407\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"407\" x2=\"480\" y2=\"407\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"latent-sft-papers\">\n    <rect x=\"600\" y=\"395\" width=\"320\" height=\"90\" class=\"sft-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"410\" class=\"text\">Coconut (Hao et al., 2024)</text>\n    <text x=\"610\" y=\"423\" class=\"text\">CCoT (Cheng and Van Durme, 2024)</text>\n    <text x=\"610\" y=\"436\" class=\"text\">CODI (Shen et al., 2025c)</text>\n    <text x=\"610\" y=\"449\" class=\"text\">Token Assorted (Su et al., 2025)</text>\n    <text x=\"610\" y=\"462\" class=\"text\">SoftCoT (Xu et al., 2025b)</text>\n    <text x=\"610\" y=\"475\" class=\"text\">Heima (Shen et al., 2025a)</text>\n    <text x=\"760\" y=\"410\" class=\"text\">Implicit CoT (Deng et al., 2024)</text>\n    <text x=\"760\" y=\"423\" class=\"text\">LightThinker (Zhang et al., 2025a)</text>\n  </g>\n  <line x1=\"580\" y1=\"407\" x2=\"600\" y2=\"407\" class=\"connector\"/>\n\n  <!-- III. Efficient Reasoning with RL -->\n  <rect x=\"270\" y=\"505\" width=\"180\" height=\"30\" class=\"main-box\" rx=\"5\"/>\n  <text x=\"360\" y=\"524\" text-anchor=\"middle\" class=\"text-main\">III. Efficient Reasoning with RL</text>\n  <line x1=\"240\" y1=\"520\" x2=\"270\" y2=\"520\" class=\"connector\"/>\n  \n  <!-- 1. With Length Reward -->\n  <rect x=\"480\" y=\"495\" width=\"100\" height=\"25\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"512\" text-anchor=\"middle\" class=\"text\">With Length Reward</text>\n  <line x1=\"450\" y1=\"520\" x2=\"470\" y2=\"520\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"520\" x2=\"470\" y2=\"507\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"507\" x2=\"480\" y2=\"507\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"rl-length-papers\">\n    <rect x=\"600\" y=\"495\" width=\"320\" height=\"65\" class=\"rl-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"510\" class=\"text\">O1-Pruner (Luo et al., 2025a)</text>\n    <text x=\"610\" y=\"523\" class=\"text\">Training (Arora and Zanette, 2025)</text>\n    <text x=\"610\" y=\"536\" class=\"text\">L1 (Aggarwal and Welleck, 2025)</text>\n    <text x=\"610\" y=\"549\" class=\"text\">Kimi-1.5 (Team et al., 2025)</text>\n    <text x=\"760\" y=\"510\" class=\"text\">DAST (Shen et al., 2025b)</text>\n    <text x=\"760\" y=\"523\" class=\"text\">Demystifying (Yeo et al., 2025)</text>\n  </g>\n  <line x1=\"580\" y1=\"507\" x2=\"600\" y2=\"507\" class=\"connector\"/>\n  \n  <!-- 2. Without Length Reward -->\n  <rect x=\"480\" y=\"570\" width=\"100\" height=\"35\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"583\" text-anchor=\"middle\" class=\"text\">Without Length</text>\n  <text x=\"530\" y=\"595\" text-anchor=\"middle\" class=\"text\">Reward</text>\n  <line x1=\"470\" y1=\"520\" x2=\"470\" y2=\"587\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"587\" x2=\"480\" y2=\"587\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"rl-without-papers\">\n    <rect x=\"600\" y=\"570\" width=\"320\" height=\"50\" class=\"rl-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"585\" class=\"text\">Meta-RL (MRT) (Qu et al., 2025b)</text>\n    <text x=\"610\" y=\"598\" class=\"text\">IBPO (Yu et al., 2025b)</text>\n    <text x=\"610\" y=\"611\" class=\"text\">Overthink (Chen et al., 2025c)</text>\n    <text x=\"760\" y=\"585\" class=\"text\">Dr.GRPO (Liu et al., 2025c)</text>\n  </g>\n  <line x1=\"580\" y1=\"587\" x2=\"600\" y2=\"587\" class=\"connector\"/>\n\n  <!-- IV. Efficient Reasoning during Pretraining -->\n  <rect x=\"270\" y=\"640\" width=\"180\" height=\"30\" class=\"main-box\" rx=\"5\"/>\n  <text x=\"360\" y=\"659\" text-anchor=\"middle\" class=\"text-main\">IV. Efficient Reasoning during Pretraining</text>\n  <line x1=\"240\" y1=\"655\" x2=\"270\" y2=\"655\" class=\"connector\"/>\n  \n  <!-- 1. Latent-space Pretraining -->\n  <rect x=\"480\" y=\"630\" width=\"100\" height=\"35\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"643\" text-anchor=\"middle\" class=\"text\">Latent-space</text>\n  <text x=\"530\" y=\"655\" text-anchor=\"middle\" class=\"text\">Pretraining</text>\n  <line x1=\"450\" y1=\"655\" x2=\"470\" y2=\"655\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"655\" x2=\"470\" y2=\"647\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"647\" x2=\"480\" y2=\"647\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"latent-pretrain-papers\">\n    <rect x=\"600\" y=\"630\" width=\"320\" height=\"50\" class=\"pretrain-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"645\" class=\"text\">BLT (Pagnoni et al., 2024)</text>\n    <text x=\"610\" y=\"658\" class=\"text\">LCMs (The et al., 2024)</text>\n    <text x=\"610\" y=\"671\" class=\"text\">CoCoMix (Tack et al., 2025)</text>\n    <text x=\"760\" y=\"645\" class=\"text\">LTMs (Kong et al., 2025)</text>\n  </g>\n  <line x1=\"580\" y1=\"647\" x2=\"600\" y2=\"647\" class=\"connector\"/>\n  \n  <!-- 2. Subquadratic Attention -->\n  <rect x=\"480\" y=\"690\" width=\"100\" height=\"35\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"703\" text-anchor=\"middle\" class=\"text\">Subquadratic</text>\n  <text x=\"530\" y=\"715\" text-anchor=\"middle\" class=\"text\">Attention</text>\n  <line x1=\"470\" y1=\"655\" x2=\"470\" y2=\"707\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"707\" x2=\"480\" y2=\"707\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"subquad-attention-papers\">\n    <rect x=\"600\" y=\"690\" width=\"320\" height=\"90\" class=\"pretrain-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"705\" class=\"text\">Lightning Attention/2 (Qin et al., 2024e)</text>\n    <text x=\"610\" y=\"718\" class=\"text\">LASP-2 (Sun et al., 2025b)</text>\n    <text x=\"610\" y=\"731\" class=\"text\">GLA (Yang et al., 2023)</text>\n    <text x=\"610\" y=\"744\" class=\"text\">Gated DeltaNet (Yang et al., 2024d)</text>\n    <text x=\"610\" y=\"757\" class=\"text\">MoM (Du et al., 2025)</text>\n    <text x=\"610\" y=\"770\" class=\"text\">Mamba-2 (Dao and Gu, 2024)</text>\n    <text x=\"760\" y=\"705\" class=\"text\">RWKV-7 (Peng et al., 2025a)</text>\n    <text x=\"760\" y=\"718\" class=\"text\">NSA (Yuan et al., 2025a)</text>\n    <text x=\"760\" y=\"731\" class=\"text\">MoBA (Lu et al., 2025a)</text>\n  </g>\n  <line x1=\"580\" y1=\"707\" x2=\"600\" y2=\"707\" class=\"connector\"/>\n  \n  <!-- 3. Linearization -->\n  <rect x=\"480\" y=\"790\" width=\"100\" height=\"25\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"530\" y=\"807\" text-anchor=\"middle\" class=\"text\">Linearization</text>\n  <line x1=\"470\" y1=\"655\" x2=\"470\" y2=\"802\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"802\" x2=\"480\" y2=\"802\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"linearization-papers\">\n    <rect x=\"600\" y=\"790\" width=\"320\" height=\"40\" class=\"pretrain-box\" rx=\"3\"/>\n    <text x=\"610\" y=\"805\" class=\"text\">Liger (Lan et al., 2025)</text>\n    <text x=\"610\" y=\"818\" class=\"text\">Llamba (Bick et al., 2025)</text>\n    <text x=\"760\" y=\"805\" class=\"text\">LoLCATs (Zhang et al., 2024a)</text>\n  </g>\n  <line x1=\"580\" y1=\"802\" x2=\"600\" y2=\"802\" class=\"connector\"/>\n\n  <!-- V. Efficient Reasoning with Subquadratic Attention -->\n  <rect x=\"270\" y=\"850\" width=\"180\" height=\"50\" class=\"main-box\" rx=\"5\"/>\n  <text x=\"360\" y=\"869\" text-anchor=\"middle\" class=\"text-main\">V. Efficient Reasoning with</text>\n  <text x=\"360\" y=\"885\" text-anchor=\"middle\" class=\"text-main\">Subquadratic Attention</text>\n  <line x1=\"240\" y1=\"875\" x2=\"270\" y2=\"875\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"subquad-reasoning-papers\">\n    <rect x=\"480\" y=\"850\" width=\"440\" height=\"40\" class=\"subquad-box\" rx=\"3\"/>\n    <text x=\"490\" y=\"865\" class=\"text\">TSF (Paliotta et al., 2025)</text>\n    <text x=\"490\" y=\"878\" class=\"text\">CRQs (Yehudai et al., 2025)</text>\n    <text x=\"650\" y=\"865\" class=\"text\">Cosmos-Reason1 (Azzolini et al., 2025)</text>\n  </g>\n  <line x1=\"450\" y1=\"875\" x2=\"480\" y2=\"875\" class=\"connector\"/>\n\n  <!-- VI. Future Directions -->\n  <rect x=\"270\" y=\"920\" width=\"180\" height=\"30\" class=\"main-box\" rx=\"5\"/>\n  <text x=\"360\" y=\"939\" text-anchor=\"middle\" class=\"text-main\">VI. Future Directions</text>\n  <line x1=\"240\" y1=\"935\" x2=\"270\" y2=\"935\" class=\"connector\"/>\n  \n  <!-- 1. Efficient Multimodality -->\n  <rect x=\"480\" y=\"900\" width=\"120\" height=\"35\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"540\" y=\"913\" text-anchor=\"middle\" class=\"text\">Efficient Multimodality</text>\n  <text x=\"540\" y=\"925\" text-anchor=\"middle\" class=\"text\">and Video Reasoning</text>\n  <line x1=\"450\" y1=\"935\" x2=\"470\" y2=\"935\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"935\" x2=\"470\" y2=\"917\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"917\" x2=\"480\" y2=\"917\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"multimodal-papers\">\n    <rect x=\"620\" y=\"900\" width=\"300\" height=\"40\" class=\"future-box\" rx=\"3\"/>\n    <text x=\"630\" y=\"915\" class=\"text\">SCoT (Xiang et al., 2025)</text>\n    <text x=\"630\" y=\"928\" class=\"text\">AL-CoTD (Peng et al., 2025b)</text>\n    <text x=\"770\" y=\"915\" class=\"text\">Heima (Shen et al., 2025a)</text>\n  </g>\n  <line x1=\"600\" y1=\"917\" x2=\"620\" y2=\"917\" class=\"connector\"/>\n  \n  <!-- 2. Efficient Test-time Scaling -->\n  <rect x=\"480\" y=\"950\" width=\"120\" height=\"35\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"540\" y=\"963\" text-anchor=\"middle\" class=\"text\">Efficient Test-time</text>\n  <text x=\"540\" y=\"975\" text-anchor=\"middle\" class=\"text\">Scaling &amp; Infinity Thinking</text>\n  <line x1=\"470\" y1=\"935\" x2=\"470\" y2=\"967\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"967\" x2=\"480\" y2=\"967\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"test-time-papers\">\n    <rect x=\"620\" y=\"950\" width=\"300\" height=\"30\" class=\"future-box\" rx=\"3\"/>\n    <text x=\"630\" y=\"965\" class=\"text\">Self-Calibration (Huang et al., 2025a)</text>\n    <text x=\"630\" y=\"975\" class=\"text\">Dynamic self-consistency (Wan et al., 2024)</text>\n  </g>\n  <line x1=\"600\" y1=\"967\" x2=\"620\" y2=\"967\" class=\"connector\"/>\n  \n  <!-- 3. Efficient and Trustworthy -->\n  <rect x=\"480\" y=\"990\" width=\"120\" height=\"35\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"540\" y=\"1003\" text-anchor=\"middle\" class=\"text\">Efficient and</text>\n  <text x=\"540\" y=\"1015\" text-anchor=\"middle\" class=\"text\">Trustworthy Reasoning</text>\n  <line x1=\"470\" y1=\"935\" x2=\"470\" y2=\"1007\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"1007\" x2=\"480\" y2=\"1007\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"trustworthy-papers\">\n    <rect x=\"620\" y=\"990\" width=\"300\" height=\"30\" class=\"future-box\" rx=\"3\"/>\n    <text x=\"630\" y=\"1005\" class=\"text\">Deliberative Alignment (Guan et al., 2024)</text>\n    <text x=\"770\" y=\"1005\" class=\"text\">X-Boundary (Lu et al., 2025b)</text>\n  </g>\n  <line x1=\"600\" y1=\"1007\" x2=\"620\" y2=\"1007\" class=\"connector\"/>\n  \n  <!-- 4. Efficient Reasoning Applications -->\n  <rect x=\"480\" y=\"1030\" width=\"120\" height=\"35\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"540\" y=\"1043\" text-anchor=\"middle\" class=\"text\">Efficient Reasoning</text>\n  <text x=\"540\" y=\"1055\" text-anchor=\"middle\" class=\"text\">Applications</text>\n  <line x1=\"470\" y1=\"935\" x2=\"470\" y2=\"1047\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"1047\" x2=\"480\" y2=\"1047\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"applications-papers\">\n    <rect x=\"620\" y=\"1030\" width=\"300\" height=\"30\" class=\"future-box\" rx=\"3\"/>\n    <text x=\"630\" y=\"1045\" class=\"text\">Chain-of-Retrieval (Wang et al., 2025a)</text>\n    <text x=\"630\" y=\"1055\" class=\"text\">Overthinking in Agent (Cuadron et al., 2025)</text>\n  </g>\n  <line x1=\"600\" y1=\"1047\" x2=\"620\" y2=\"1047\" class=\"connector\"/>\n  \n  <!-- 5. Evaluation and Benchmark -->\n  <rect x=\"480\" y=\"1070\" width=\"120\" height=\"35\" class=\"main-box\" rx=\"3\"/>\n  <text x=\"540\" y=\"1083\" text-anchor=\"middle\" class=\"text\">Evaluation and</text>\n  <text x=\"540\" y=\"1095\" text-anchor=\"middle\" class=\"text\">Benchmark</text>\n  <line x1=\"470\" y1=\"935\" x2=\"470\" y2=\"1087\" class=\"connector\"/>\n  <line x1=\"470\" y1=\"1087\" x2=\"480\" y2=\"1087\" class=\"connector\"/>\n  \n  <g class=\"component\" data-component=\"evaluation-papers\">\n    <rect x=\"620\" y=\"1070\" width=\"300\" height=\"30\" class=\"future-box\" rx=\"3\"/>\n    <text x=\"630\" y=\"1085\" class=\"text\">Overthinking (Chen et al., 2025c)</text>\n    <text x=\"770\" y=\"1085\" class=\"text\">DNA Bench (Hashemi et al., 2025)</text>\n  </g>\n  <line x1=\"600\" y1=\"1087\" x2=\"620\" y2=\"1087\" class=\"connector\"/>\n</svg>",
  "componentExplanations": {
    "length-budgeting": {
      "title": "Length Budgeting",
      "explanation": "Methods that control the computational budget by limiting the length of reasoning chains. These approaches dynamically adjust how many reasoning steps to perform based on problem difficulty, avoiding overthinking on simple problems while allocating more compute to complex ones."
    },
    "system-switch": {
      "title": "System Switch (System 1 ↔ System 2)",
      "explanation": "Techniques that switch between fast, intuitive reasoning (System 1) and slow, deliberative reasoning (System 2). These methods intelligently route queries to appropriate reasoning modes, using lightweight models for simple tasks and invoking heavy reasoning only when necessary."
    },
    "model-switch": {
      "title": "Model Switch",
      "explanation": "Approaches that dynamically select or switch between different models during inference. This includes using smaller models for initial processing and larger models for refinement, or routing to specialized models based on task requirements."
    },
    "parallel-search": {
      "title": "Parallel Search",
      "explanation": "Methods that explore multiple reasoning paths simultaneously rather than sequentially. These include best-of-N sampling, tree-based search, and self-calibration techniques that generate and evaluate multiple candidate solutions in parallel."
    },
    "chain-compression": {
      "title": "Reasoning Chain Compression",
      "explanation": "Supervised fine-tuning methods that learn to compress verbose reasoning chains into more efficient representations. These approaches train models to skip redundant steps, use shortcuts, or internalize reasoning patterns to reduce inference-time computation."
    },
    "latent-sft": {
      "title": "Latent-Space SFT",
      "explanation": "Fine-tuning approaches that operate in latent space rather than token space. These methods learn compressed representations of reasoning processes, allowing models to perform implicit reasoning without generating explicit chain-of-thought tokens."
    },
    "rl-length": {
      "title": "RL with Length Reward",
      "explanation": "Reinforcement learning methods that explicitly reward shorter reasoning chains. These approaches balance accuracy with efficiency by incorporating length penalties into the reward function, encouraging models to find concise solutions."
    },
    "rl-without-length": {
      "title": "RL without Length Reward",
      "explanation": "RL methods focused purely on improving reasoning quality without explicit length constraints. These approaches may still achieve efficiency gains through emergent optimization of reasoning strategies, even without direct length incentives."
    },
    "latent-pretrain": {
      "title": "Latent-space Pretraining",
      "explanation": "Pretraining methods that learn reasoning in compressed latent representations from the start. Rather than learning token-by-token generation, these approaches train models to manipulate abstract reasoning states efficiently."
    },
    "subquad-attention": {
      "title": "Subquadratic Attention",
      "explanation": "Architectural innovations that reduce the quadratic complexity of attention mechanisms. These include linear attention variants, state-space models, and hybrid architectures that maintain reasoning capabilities while dramatically reducing computational costs."
    },
    "linearization": {
      "title": "Linearization",
      "explanation": "Methods that convert transformer architectures to linear-complexity alternatives. These approaches use techniques like diagonal approximations or recurrent formulations to achieve O(n) complexity while preserving reasoning abilities."
    },
    "subquad-reasoning": {
      "title": "Efficient Reasoning with Subquadratic Attention",
      "explanation": "Specialized reasoning methods designed for subquadratic architectures. These techniques adapt chain-of-thought and other reasoning strategies to work efficiently with linear attention mechanisms and state-space models."
    },
    "multimodal": {
      "title": "Efficient Multimodality and Video Reasoning",
      "explanation": "Future direction focusing on extending efficient reasoning to multimodal inputs. This includes handling video sequences, cross-modal reasoning, and efficient processing of high-dimensional sensory data alongside text."
    },
    "test-time-scaling": {
      "title": "Efficient Test-time Scaling",
      "explanation": "Methods for dynamically scaling computation at inference time. These include adaptive computation techniques and 'infinity thinking' approaches that can allocate arbitrary amounts of compute based on problem difficulty and available resources."
    },
    "trustworthy": {
      "title": "Efficient and Trustworthy Reasoning",
      "explanation": "Approaches that maintain reasoning transparency and reliability while improving efficiency. This includes methods for verifiable reasoning chains, uncertainty quantification, and maintaining interpretability in compressed reasoning."
    },
    "applications": {
      "title": "Efficient Reasoning Applications",
      "explanation": "Domain-specific applications of efficient reasoning. This includes retrieval-augmented reasoning, agent-based systems, and specialized applications where reasoning efficiency is critical for real-world deployment."
    },
    "evaluation": {
      "title": "Evaluation and Benchmark",
      "explanation": "Benchmarks and evaluation frameworks specifically designed for efficient reasoning. These measure not just accuracy but also computational efficiency, overthinking detection, and the quality-efficiency tradeoff."
    },
    "length-budgeting-papers": {
      "title": "Length Budgeting Papers",
      "explanation": "Collection of papers implementing various approaches to control reasoning length through budget allocation strategies."
    },
    "system-switch-papers": {
      "title": "System Switch Papers",
      "explanation": "Papers exploring dynamic switching between fast and slow reasoning systems based on task complexity."
    },
    "model-switch-papers": {
      "title": "Model Switch Papers",
      "explanation": "Research on dynamically selecting or switching between different model sizes and architectures during inference."
    },
    "parallel-search-papers": {
      "title": "Parallel Search Papers",
      "explanation": "Studies on exploring multiple reasoning paths simultaneously for improved efficiency and accuracy."
    },
    "chain-compression-papers": {
      "title": "Chain Compression Papers",
      "explanation": "Work on compressing verbose reasoning chains through supervised fine-tuning approaches."
    },
    "latent-sft-papers": {
      "title": "Latent-Space SFT Papers",
      "explanation": "Research on fine-tuning models to reason in compressed latent representations rather than explicit tokens."
    },
    "rl-length-papers": {
      "title": "RL with Length Reward Papers",
      "explanation": "Studies using reinforcement learning with explicit length penalties to optimize reasoning efficiency."
    },
    "rl-without-papers": {
      "title": "RL without Length Reward Papers",
      "explanation": "RL approaches focusing on reasoning quality without explicit length constraints."
    },
    "latent-pretrain-papers": {
      "title": "Latent-space Pretraining Papers",
      "explanation": "Work on pretraining models to manipulate abstract reasoning states from the start."
    },
    "subquad-attention-papers": {
      "title": "Subquadratic Attention Papers",
      "explanation": "Research on reducing attention complexity while maintaining reasoning capabilities."
    },
    "linearization-papers": {
      "title": "Linearization Papers",
      "explanation": "Studies on converting transformers to linear-complexity architectures."
    },
    "subquad-reasoning-papers": {
      "title": "Efficient Reasoning with Subquadratic Attention Papers",
      "explanation": "Work on adapting reasoning strategies for subquadratic architectures."
    },
    "multimodal-papers": {
      "title": "Multimodal Efficiency Papers",
      "explanation": "Research on extending efficient reasoning to multimodal and video inputs."
    },
    "test-time-papers": {
      "title": "Test-time Scaling Papers",
      "explanation": "Studies on dynamic computation allocation during inference."
    },
    "trustworthy-papers": {
      "title": "Trustworthy Reasoning Papers",
      "explanation": "Work on maintaining transparency and reliability in efficient reasoning."
    },
    "applications-papers": {
      "title": "Application Papers",
      "explanation": "Domain-specific applications of efficient reasoning techniques."
    },
    "evaluation-papers": {
      "title": "Evaluation Papers",
      "explanation": "Benchmarks and metrics for assessing reasoning efficiency."
    }
  },
  "componentQuizzes": {
    "length-budgeting-papers": {
      "id": "length-budgeting-papers",
      "title": "Length Budgeting Papers",
      "concepts": [
        {
          "id": "tale-concept",
          "title": "TALE Budget Estimation",
          "question": "How does TALE estimate the optimal token budget for model generation?",
          "answer": "TALE uses zero-shot prompting to estimate an optimal token budget which constrains model generation",
          "canvasHeight": 100
        },
        {
          "id": "sketch-concept",
          "title": "Sketch-of-Thought Paradigms",
          "question": "What are the three adaptive paradigms used by Sketch-of-Thought to reduce verbosity in intermediate reasoning steps?",
          "answer": "Conceptual Chaining, Chunked Symbolism, and Expert Lexicon",
          "canvasHeight": 100
        },
        {
          "id": "planning-tokens-concept",
          "title": "Planning Tokens Hierarchy",
          "question": "How do planning tokens enhance language model reasoning according to Wang et al. (2023b)?",
          "answer": "By incorporating planning tokens at the start of each reasoning step in a hierarchical approach",
          "canvasHeight": 100
        },
        {
          "id": "chain-draft-concept",
          "title": "Chain-of-Draft Efficiency",
          "question": "What is the key difference between Chain-of-Draft and traditional CoT?",
          "answer": "Chain-of-Draft encourages models to generate concise, minimal intermediate reasoning steps rather than token-heavy explanations",
          "canvasHeight": 120
        },
        {
          "id": "s1-concept",
          "title": "S1 Budget Forcing",
          "question": "How does S1 directly control thinking length?",
          "answer": "Uses a budget-forcing strategy by appending an end-of-thinking token delimiter to force the thinking process to end",
          "canvasHeight": 100
        },
        {
          "id": "safechain-concept",
          "title": "SafeChain Decoding",
          "question": "What are the two decoding strategies proposed by SafeChain to control thinking?",
          "answer": "ZeroThink (forces model to start response without any thought) and LessThink (forces a short thought process)",
          "canvasHeight": 120
        },
        {
          "id": "dsc-concept",
          "title": "DSC Resource Allocation",
          "question": "How does Difficulty-Adaptive Self-Consistency (DSC) allocate inference resources?",
          "answer": "Evaluates query difficulty using the LLM itself to dynamically allocate inference resources",
          "canvasHeight": 100
        },
        {
          "id": "dynasor-concept",
          "title": "Dynasor Compute Allocation",
          "question": "What basis does Dynasor use to allocate compute during multi-path reasoning?",
          "answer": "Allocates compute based on model certainty, assigning more resources to hard queries",
          "canvasHeight": 100
        },
        {
          "id": "tsp-concept",
          "title": "Thought Switching Penalty",
          "question": "What does TSP discourage to improve reasoning efficiency?",
          "answer": "Discourages premature transitions between thoughts which may cause superficial but lengthy reasoning traces",
          "canvasHeight": 100
        }
      ]
    },
    "system-switch-papers": {
      "id": "system-switch-papers",
      "title": "System Switch Papers",
      "concepts": [
        {
          "id": "dualformer-concept",
          "title": "Dualformer Integration",
          "question": "How does Dualformer integrate the dual process through its training strategy?",
          "answer": "Uses a randomized reasoning trace training strategy that randomly drops certain parts of the reasoning traces",
          "canvasHeight": 100
        },
        {
          "id": "system1x-concept",
          "title": "System 1.x Controller",
          "question": "What role does the controller play in System 1.x when handling maze tasks?",
          "answer": "Assesses maze difficulty to allow the model to alternate among different systems based on user-defined parameters for smoother cognitive resource allocation",
          "canvasHeight": 120
        },
        {
          "id": "fast-concept",
          "title": "FaST Visual Reasoning",
          "question": "What factors does FaST's switching adapter use to transition between Systems 1 and 2 for visual reasoning?",
          "answer": "Task complexity factors like visual uncertainty and invisibility",
          "canvasHeight": 100
        },
        {
          "id": "halusearch-concept",
          "title": "HaluSearch Learning",
          "question": "How does HaluSearch learn to switch between System 1 and System 2?",
          "answer": "Leverages model performance on specific instances to construct supervised labels for both instance-level and step-level switching under MCTS",
          "canvasHeight": 120
        },
        {
          "id": "dyna-think-concept",
          "title": "Dyna-Think Autonomy",
          "question": "What makes Dyna-Think's dynamic thinking mechanism 'training-free'?",
          "answer": "The model autonomously determines 'Slow' reasoning based on generation consistency and complexity of thought processes",
          "canvasHeight": 100
        }
      ]
    },
    "model-switch-papers": {
      "id": "model-switch-papers",
      "title": "Model Switch Papers",
      "concepts": [
        {
          "id": "bild-concept",
          "title": "BiLD Dual Model",
          "question": "How does BiLD balance speed and quality through its policies?",
          "answer": "Uses a small, fast model for initial predictions and a larger, more accurate model for corrections through fallback and rollback policies",
          "canvasHeight": 120
        },
        {
          "id": "eagle-concept",
          "title": "EAGLE Feature-Level",
          "question": "What key transition does EAGLE make to enhance inference?",
          "answer": "Transitions speculative sampling from the token level to the feature level",
          "canvasHeight": 100
        },
        {
          "id": "eagle2-concept",
          "title": "EAGLE-2 Dynamic Trees",
          "question": "What innovation does EAGLE-2 introduce for speculative decoding?",
          "answer": "Context-aware dynamic draft trees that adjust token acceptance rates based on confidence scores",
          "canvasHeight": 100
        },
        {
          "id": "medusa-concept",
          "title": "MEDUSA Multi-Token",
          "question": "How does MEDUSA reduce sequential decoding steps?",
          "answer": "Incorporates additional decoding heads that predict multiple tokens simultaneously with a tree-based attention mechanism to concurrently generate and verify candidate continuations",
          "canvasHeight": 120
        },
        {
          "id": "layerskip-concept",
          "title": "LayerSkip Early Exit",
          "question": "What two techniques does LayerSkip combine to speed up inference?",
          "answer": "Layer dropout combined with early exit loss, allowing predictions at shallower layers with self-speculative decoding for verification",
          "canvasHeight": 120
        },
        {
          "id": "zooter-concept",
          "title": "Zooter Routing",
          "question": "How does Zooter's reward-guided routing method determine which LLM to use?",
          "answer": "Leverages distilled rewards from training queries to train a specialized routing function that directs each query to the LLM with the most pertinent expertise",
          "canvasHeight": 120
        },
        {
          "id": "routellm-concept",
          "title": "RouteLLM Balance",
          "question": "What balance does RouteLLM strike through its dynamic routing?",
          "answer": "Dynamically routes queries between robust and weaker language models to achieve optimal balance between performance and cost effectiveness",
          "canvasHeight": 120
        },
        {
          "id": "mixllm-concept",
          "title": "MixLLM Decision Making",
          "question": "How does MixLLM enhance its selection of optimal LLM candidates?",
          "answer": "Enhances query embeddings using tag knowledge, employs lightweight predictors to assess quality and cost per model, and uses a meta decision maker",
          "canvasHeight": 120
        }
      ]
    },
    "parallel-search-papers": {
      "id": "parallel-search-papers",
      "title": "Parallel Search Papers",
      "concepts": [
        {
          "id": "sbon-concept",
          "title": "SBoN Early Halting",
          "question": "How does SBoN achieve comparable performance while reducing computational demands?",
          "answer": "Evaluates partial responses and halts those that are unlikely to yield high-quality completions",
          "canvasHeight": 100
        },
        {
          "id": "treebon-concept",
          "title": "TreeBoN Hierarchical",
          "question": "What combination does TreeBoN use and how does it manage candidates?",
          "answer": "Combines speculative tree-search with Best-of-N sampling, generating candidates in hierarchical tree structure and pruning low-quality ones early using weighted implicit reward",
          "canvasHeight": 120
        },
        {
          "id": "stbon-concept",
          "title": "STBoN Truncation",
          "question": "How does STBoN identify when to truncate suboptimal candidates?",
          "answer": "Identifies the earliest estimation time when samples become distinct and employs a buffer window along with hidden state consistency",
          "canvasHeight": 120
        },
        {
          "id": "self-calibration-concept",
          "title": "Self-Calibration Distillation",
          "question": "What does Self-Calibration distill into the model and what benefit does this provide?",
          "answer": "Distills self-consistency-derived confidence into the model, enabling strategies like early stopping and eliminating the need for external reward models",
          "canvasHeight": 120
        },
        {
          "id": "metareasoner-concept",
          "title": "MetaReasoner Formulation",
          "question": "What formulation does MetaReasoner use for its strategies like restarting or refining?",
          "answer": "Uses a contextual multi-armed bandit formulation",
          "canvasHeight": 100
        },
        {
          "id": "tpo-concept",
          "title": "TPO Recursive Approach",
          "question": "What does TPO's recursive approach achieve compared to training-aware methods?",
          "answer": "Revises parallel samples to align model performance at test time, achieving comparable results to training-aware methods",
          "canvasHeight": 100
        }
      ]
    },
    "chain-compression-papers": {
      "id": "chain-compression-papers",
      "title": "Chain Compression Papers",
      "concepts": [
        {
          "id": "tokenskip-concept",
          "title": "TokenSkip Analysis",
          "question": "How does TokenSkip achieve controllable compression of CoT sequences?",
          "answer": "Analyzes token importance in CoT outputs and selectively omits less important tokens",
          "canvasHeight": 100
        },
        {
          "id": "spirit-ft-concept",
          "title": "SPIRIT-FT Metric",
          "question": "What metric does SPIRIT-FT use to identify critical reasoning steps?",
          "answer": "Uses perplexity as a metric - a step is deemed critical if its removal significantly increases perplexity",
          "canvasHeight": 100
        },
        {
          "id": "lm-skip-concept",
          "title": "LM-Skip Environment",
          "question": "How does LM-Skip induce step-skipping behavior?",
          "answer": "Designs a controlled training environment that instructs models to produce reasoning sequences under a step constraint",
          "canvasHeight": 100
        },
        {
          "id": "distill-system2-concept",
          "title": "Distill System 2",
          "question": "When does Yu et al.'s approach fine-tune models to omit intermediate generation steps?",
          "answer": "For samples that are sufficiently confident",
          "canvasHeight": 100
        },
        {
          "id": "c3ot-concept",
          "title": "C3ot Compression",
          "question": "How does C3ot preserve key information and learn compression?",
          "answer": "Employs GPT-4 as a compressor to preserve key information, then fine-tunes to learn the relationship between long and short CoTs",
          "canvasHeight": 120
        },
        {
          "id": "self-training-concept",
          "title": "Self-Training Distillation",
          "question": "What two techniques does Munkhbat et al.'s self-training use before applying SFT?",
          "answer": "Best-of-N sampling and few-shot conditioning to build concise reasoning paths",
          "canvasHeight": 100
        }
      ]
    },
    "latent-sft-papers": {
      "id": "latent-sft-papers",
      "title": "Latent-Space SFT Papers",
      "concepts": [
        {
          "id": "coconut-concept",
          "title": "Coconut Continuous Thought",
          "question": "How does Coconut (Chain of Continuous Thought) replace traditional CoT?",
          "answer": "Uses the model's last hidden state as a continuous representation of reasoning, feeding it back into the model as input for subsequent reasoning steps",
          "canvasHeight": 120
        },
        {
          "id": "ccot-concept",
          "title": "CCoT Compression",
          "question": "How does CCoT reduce computational cost and enhance throughput?",
          "answer": "Fine-tunes the model to produce compressed representations of reasoning chains instead of full-length sequences, approximating complete chains with fewer tokens",
          "canvasHeight": 120
        },
        {
          "id": "codi-concept",
          "title": "CODI Distillation",
          "question": "What does CODI align between teacher and student models?",
          "answer": "Aligns hidden activations of specific tokens between a teacher model using explicit CoT and a student model using implicit CoT",
          "canvasHeight": 120
        },
        {
          "id": "token-assorted-concept",
          "title": "Token Assorted Mixing",
          "question": "How does Token Assorted abstract initial reasoning steps?",
          "answer": "Mixes latent discrete tokens from VQ-VAE with text tokens to abstract the initial reasoning steps while retaining essential information",
          "canvasHeight": 120
        },
        {
          "id": "softcot-concept",
          "title": "SoftCoT Assistant",
          "question": "How does SoftCoT approach continuous-space reasoning differently?",
          "answer": "Utilizes an assistant model that generates 'soft thought tokens' for the LLM",
          "canvasHeight": 100
        },
        {
          "id": "lightthinker-concept",
          "title": "LightThinker Compression",
          "question": "How does LightThinker enhance reasoning efficiency?",
          "answer": "Dynamically compresses intermediate steps into concise latent representations",
          "canvasHeight": 100
        },
        {
          "id": "heima-concept",
          "title": "Heima Architecture",
          "question": "How does Heima reduce verbosity in both text and multimodal tasks?",
          "answer": "The Heima Encoder compresses intermediate steps into a single token, and the Heima Decoder reconstructs the reasoning process from these tokens",
          "canvasHeight": 120
        },
        {
          "id": "implicit-cot-concept",
          "title": "Implicit CoT",
          "question": "What is the key characteristic of Implicit CoT by Deng et al.?",
          "answer": "Performs reasoning in latent space without generating explicit chain-of-thought tokens",
          "canvasHeight": 100
        }
      ]
    },
    "rl-length-papers": {
      "id": "rl-length-papers",
      "title": "RL with Length Reward Papers",
      "concepts": [
        {
          "id": "o1-pruner-concept",
          "title": "O1-Pruner Method",
          "question": "How does O1-Pruner begin its efficient fine-tuning method?",
          "answer": "Begins by estimating the LLM's baseline performance through presampling from its reference model",
          "canvasHeight": 100
        },
        {
          "id": "arora-zanette-concept",
          "title": "Normalized Length Penalty",
          "question": "How does Arora and Zanette's length penalty ensure correct responses are preferred?",
          "answer": "Introduces a length penalty normalized in the per-prompt group, ensuring correct responses are always preferred over incorrect ones regardless of token count",
          "canvasHeight": 120
        },
        {
          "id": "lcpo-concept",
          "title": "LCPO Target Control",
          "question": "How does LCPO control length budget through prompts and rewards?",
          "answer": "Introduces target length instruction in the prompt ('Think for ngold tokens') and designs a target-aware length reward that penalizes length violation",
          "canvasHeight": 120
        },
        {
          "id": "kimi-concept",
          "title": "Kimi 1.5 Observation",
          "question": "What phenomenon does Kimi 1.5 observe and how does it address it?",
          "answer": "Observes the overthinking phenomenon and introduces a length reward to restrain the rapid growth of token length",
          "canvasHeight": 100
        },
        {
          "id": "dast-concept",
          "title": "DAST Adaptive Thinking",
          "question": "How does DAST empower models to modulate CoT length?",
          "answer": "Introduces Difficulty-Adaptive Slow-Thinking that allows models to autonomously modulate CoT length based on problem complexity",
          "canvasHeight": 100
        },
        {
          "id": "demystifying-concept",
          "title": "Demystifying Paradox",
          "question": "What paradoxical finding did Yeo et al. reveal about extremely long CoT reasoning?",
          "answer": "Extremely long CoT reasoning approaching context limits paradoxically reduces accuracy; they proposed a cosine reward function for intuitive guidance",
          "canvasHeight": 120
        }
      ]
    },
    "rl-without-papers": {
      "id": "rl-without-papers",
      "title": "RL without Length Reward Papers",
      "concepts": [
        {
          "id": "mrt-concept",
          "title": "MRT Formulation",
          "question": "How does MRT (Meta-RL) structure its test-time optimization approach?",
          "answer": "Formulates test-time optimization as a meta-reinforcement learning problem, dividing generation into episodes with model estimating answers after each episode",
          "canvasHeight": 120
        },
        {
          "id": "ibpo-concept",
          "title": "IBPO Framing",
          "question": "How does IBPO approach budget awareness differently than direct length control?",
          "answer": "Frames budget awareness as utility maximization rather than directly controlling response length",
          "canvasHeight": 100
        },
        {
          "id": "overthink-concept",
          "title": "Overthink Heuristics",
          "question": "What heuristics does Overthink employ for generating preference data?",
          "answer": "Uses First-Correct Solutions (FCS) and Greedy Diverse Solutions (GDS) to generate preference data for offline policy optimization using DPO, RPO, and SimPO",
          "canvasHeight": 120
        },
        {
          "id": "drgrpo-concept",
          "title": "Dr.GRPO Approach",
          "question": "What is Dr.GRPO's focus in reasoning without length rewards?",
          "answer": "Focuses on improving reasoning quality through group relative policy optimization without explicit length constraints",
          "canvasHeight": 100
        }
      ]
    },
    "latent-pretrain-papers": {
      "id": "latent-pretrain-papers",
      "title": "Latent-space Pretraining Papers",
      "concepts": [
        {
          "id": "blt-concept",
          "title": "BLT Processing",
          "question": "How does Byte Latent Transformer process input differently than traditional models?",
          "answer": "Processes raw bytes using dynamically sized patches rather than fixed tokens, reducing computational overhead and improving robustness",
          "canvasHeight": 120
        },
        {
          "id": "lcms-concept",
          "title": "LCMs Semantic Level",
          "question": "What serves as the primary processing units in Large Concept Models?",
          "answer": "Abstract concepts that often correspond to complete sentences or speech utterances",
          "canvasHeight": 100
        },
        {
          "id": "cocomix-concept",
          "title": "CoCoMix Integration",
          "question": "What two types of predictions does CoCoMix integrate?",
          "answer": "Integrates discrete token prediction with continuous concept vectors derived from sparse autoencoders (SAEs)",
          "canvasHeight": 100
        },
        {
          "id": "ltms-concept",
          "title": "LTMs Guidance",
          "question": "How do latent thought vectors (LTMs) influence token generation?",
          "answer": "Probabilistically guide token generation via cross-attention mechanisms",
          "canvasHeight": 100
        }
      ]
    },
    "subquad-attention-papers": {
      "id": "subquad-attention-papers",
      "title": "Subquadratic Attention Papers",
      "concepts": [
        {
          "id": "lightning-concept",
          "title": "Lightning Attention",
          "question": "What does Lightning Attention optimize to expedite processing?",
          "answer": "Optimizes I/O operations to expedite processing",
          "canvasHeight": 100
        },
        {
          "id": "lasp2-concept",
          "title": "LASP-2 Refinement",
          "question": "How does LASP-2 refine the Lightning Attention approach?",
          "answer": "Further refines by reorganizing both computational and communication workflows",
          "canvasHeight": 100
        },
        {
          "id": "gla-concept",
          "title": "GLA Gating",
          "question": "What scheme does Gated Linear Attention use to enhance sequence modeling?",
          "answer": "Leverages a data-independent gating scheme to enhance sequence modeling ability and hardware efficiency",
          "canvasHeight": 100
        },
        {
          "id": "gated-deltanet-concept",
          "title": "Gated DeltaNet Updates",
          "question": "What capability do TTT, Titans, and Gated-DeltaNet share?",
          "answer": "Propose update rules that allow models to adapt dynamically during inference",
          "canvasHeight": 100
        },
        {
          "id": "mom-concept",
          "title": "MoM Memory",
          "question": "How does MoM expand the RNN memory state?",
          "answer": "Uses 'sparse memory' with multiple memory units managed by a router module",
          "canvasHeight": 100
        },
        {
          "id": "mamba2-concept",
          "title": "Mamba-2 Integration",
          "question": "What does Mamba-2 integrate to enhance hardware efficiency?",
          "answer": "Integrates a linear attention-like mechanism",
          "canvasHeight": 100
        },
        {
          "id": "rwkv7-concept",
          "title": "RWKV-7 Method",
          "question": "What category of methods does RWKV-7 belong to?",
          "answer": "Linear RNN-based methods that have demonstrated effectiveness",
          "canvasHeight": 100
        },
        {
          "id": "nsa-concept",
          "title": "NSA Strategy",
          "question": "What hierarchical strategy does NSA adopt?",
          "answer": "Dynamic hierarchical sparse strategy combining coarse-grained token compression with fine-grained token selection",
          "canvasHeight": 100
        },
        {
          "id": "moba-concept",
          "title": "MoBA Routing",
          "question": "How does MoBA handle context and routing?",
          "answer": "Divides context into blocks and uses dynamic gating mechanism to route query tokens to the most relevant KV blocks",
          "canvasHeight": 120
        }
      ]
    },
    "linearization-papers": {
      "id": "linearization-papers",
      "title": "Linearization Papers",
      "concepts": [
        {
          "id": "liger-concept",
          "title": "Liger Modification",
          "question": "How does Liger modify pre-trained LLMs without extra parameters?",
          "answer": "Modifies pre-trained LLMs into gated linear recurrent models by adapting key matrix weights",
          "canvasHeight": 100
        },
        {
          "id": "llamba-concept",
          "title": "Llamba Distillation",
          "question": "What does Llamba achieve through MOHAWK with minimal training data?",
          "answer": "Distills Llama-3.x models into the Mamba architecture, achieving high inference throughput and efficiency",
          "canvasHeight": 100
        },
        {
          "id": "lolcats-concept",
          "title": "LoLCATs Enhancement",
          "question": "How does LoLCATs advance LLM linearization?",
          "answer": "Replaces softmax attention with trained linear approximations and enhances model quality using LoRA",
          "canvasHeight": 100
        }
      ]
    },
    "subquad-reasoning-papers": {
      "id": "subquad-reasoning-papers",
      "title": "Efficient Reasoning with Subquadratic Attention Papers",
      "concepts": [
        {
          "id": "tsf-concept",
          "title": "Think Slow Fast",
          "question": "What does TSF demonstrate about distilling Mamba models from Transformers?",
          "answer": "Demonstrates that distilling Mamba models from Transformers enables faster multi-path CoT generation under fixed compute budgets",
          "canvasHeight": 120
        },
        {
          "id": "crqs-concept",
          "title": "CRQs Expressiveness",
          "question": "What does CRQs examine regarding different model architectures?",
          "answer": "Examines the expressiveness of Transformers, RNNs, and CoT-augmented models on Compositional Reasoning Questions",
          "canvasHeight": 100
        },
        {
          "id": "cosmos-reason1-concept",
          "title": "Cosmos-Reason1 Architecture",
          "question": "What hybrid architecture does Cosmos-Reason1 use for efficient Physical AI reasoning?",
          "answer": "Uses hybrid Mamba-MLP-Transformer backbone architecture",
          "canvasHeight": 100
        }
      ]
    },
    "multimodal-papers": {
      "id": "multimodal-papers",
      "title": "Multimodal Efficiency Papers",
      "concepts": [
        {
          "id": "scot-concept",
          "title": "SCoT Decomposition",
          "question": "How does Self-structured Chain of Thought (SCoT) address multimodal reasoning issues?",
          "answer": "Decomposes reasoning tasks into atomic, semantically meaningful steps",
          "canvasHeight": 100
        },
        {
          "id": "al-cotd-concept",
          "title": "AL-CoTD Adaptation",
          "question": "How does AL-CoTD refine the reasoning process?",
          "answer": "Dynamically adjusts the length of reasoning chains according to task complexity",
          "canvasHeight": 100
        },
        {
          "id": "heima-multimodal-concept",
          "title": "Heima Multimodal",
          "question": "What capability does Heima provide for multimodal tasks?",
          "answer": "Leverages hidden latent representations to reduce verbosity in both text and multimodal tasks",
          "canvasHeight": 100
        }
      ]
    },
    "test-time-papers": {
      "id": "test-time-papers",
      "title": "Test-time Scaling Papers",
      "concepts": [
        {
          "id": "confidence-methods-concept",
          "title": "Confidence-Based Methods",
          "question": "How can inefficient reasoning be mitigated according to Huang et al.?",
          "answer": "By developing confidence-based methods to address queries of varying difficulty",
          "canvasHeight": 100
        },
        {
          "id": "adaptive-sampling-concept",
          "title": "Adaptive Sampling",
          "question": "What type of strategies do Wan et al. and others propose for test-time scaling?",
          "answer": "Adaptive sampling strategies for dynamic computation allocation",
          "canvasHeight": 100
        }
      ]
    },
    "trustworthy-papers": {
      "id": "trustworthy-papers",
      "title": "Trustworthy Reasoning Papers",
      "concepts": [
        {
          "id": "deliberative-alignment-concept",
          "title": "Deliberative Alignment",
          "question": "What approach does deliberative alignment offer for LRM safety?",
          "answer": "Offers a distinct approach to enhance LRM safety through deliberative processes",
          "canvasHeight": 100
        },
        {
          "id": "x-boundary-concept",
          "title": "X-Boundary",
          "question": "What is X-Boundary's contribution to LRM safety?",
          "answer": "Provides a distinct approach to enhance LRM safety alongside deliberative alignment",
          "canvasHeight": 100
        }
      ]
    },
    "applications-papers": {
      "id": "applications-papers",
      "title": "Application Papers",
      "concepts": [
        {
          "id": "chain-retrieval-concept",
          "title": "Chain-of-Retrieval",
          "question": "How do O1-like RAG models trained with Wang et al.'s method operate?",
          "answer": "Perform step-by-step retrieval and reasoning over relevant information before generating the final answer",
          "canvasHeight": 100
        },
        {
          "id": "agent-overthinking-concept",
          "title": "Agent Overthinking",
          "question": "What correlation did Cuadron et al. find in magnetic tasks?",
          "answer": "Elevated overthinking scores correlate negatively with performance",
          "canvasHeight": 100
        }
      ]
    },
    "evaluation-papers": {
      "id": "evaluation-papers",
      "title": "Evaluation Papers",
      "concepts": [
        {
          "id": "efficiency-metrics-concept",
          "title": "Efficiency Metrics",
          "question": "What two perspectives do Chen et al.'s efficiency metrics address?",
          "answer": "Both outcome and process perspectives",
          "canvasHeight": 100
        },
        {
          "id": "dna-bench-concept",
          "title": "DNA Bench",
          "question": "What vulnerability does DNA Bench expose in LRMs?",
          "answer": "LRMs' tendency for over-reasoning",
          "canvasHeight": 100
        }
      ]
    }
  }
}